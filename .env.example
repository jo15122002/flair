# ==============================================
# Environment Variables for Local Testing
# Copy this file to ".env" and fill in values.
# ==============================================

##############################################
# GitHub Configuration
##############################################

# GitHub personal access token with at least:
#   - repo: read (to fetch diffs and file contents)
#   - issues: write (to post comments)
#   - pull-requests: write (to post PR reviews)
GITHUB_TOKEN=your_github_token_here

# (Optional) GitHub API URL; only change if using GitHub Enterprise
GITHUB_API_URL=https://api.github.com

# Repository under test, in the format "owner/repo"
REPOSITORY=your-username/your-repo

# Pull request number to test against (for local runs)
PR_NUMBER=1

##############################################
# LLM Configuration
##############################################

# URL of your LLM inference endpoint (e.g., a llama.cpp server)
LLM_ENDPOINT=http://localhost:8080/predict

# Maximum number of characters per diff chunk sent to the LLM
DIFF_CHUNK_SIZE=10000

##############################################
# Diff Filtering
##############################################

# Comma-separated list of filename patterns to exclude from review
EXCLUDE_PATTERNS=test,tests,spec

##############################################
# Output Mode
##############################################

# If true, posts one PR Review with a summary + inline suggestions.
# If false, posts one issue comment per suggestion.
SUMMARY_MODE=false
